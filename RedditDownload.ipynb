{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Reddit using PRAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to reddit: True\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "reddit = praw.Reddit(client_id='b-rGm7WdyYgAUw',\n",
    "                     client_secret='NTUUgRLnUso2C-y8GQkdyWykmeo',\n",
    "                     user_agent='ToastMeBot')\n",
    "print(\"Connected to reddit:\", reddit.read_only) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the titles, comments, and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "# Get the top 1000 posts\n",
    "print(\"TOP\")\n",
    "for i, submission in tqdm(enumerate(reddit.subreddit('toastme').top(limit=1000)), total=1000):\n",
    "    curr = {}\n",
    "    curr['title'] = submission.title\n",
    "    curr['url'] = submission.url\n",
    "    curr['comments'] = [comment.body for comment in submission.comments if hasattr(comment, 'body')]\n",
    "    data.append(curr)\n",
    "json.dump(data, open('data.json', 'w'))\n",
    "\n",
    "# Get the top 1000 hottest posts\n",
    "print(\"HOT\")\n",
    "for i, submission in tqdm(enumerate(reddit.subreddit('toastme').hot(limit=1000)), total=1000):\n",
    "    curr = {}\n",
    "    curr['title'] = submission.title\n",
    "    curr['url'] = submission.url\n",
    "    curr['comments'] = [comment.body for comment in submission.comments if hasattr(comment, 'body')]\n",
    "    data.append(curr)\n",
    "json.dump(data, open('data.json', 'w'))\n",
    "\n",
    "# Get the top 1000 newest posts\n",
    "print(\"NEW\")\n",
    "for i, submission in tqdm(enumerate(reddit.subreddit('toastme').new(limit=1000)), total=1000):\n",
    "    curr = {}\n",
    "    curr['title'] = submission.title\n",
    "    curr['url'] = submission.url\n",
    "    curr['comments'] = [comment.body for comment in submission.comments if hasattr(comment, 'body')]\n",
    "    data.append(curr)\n",
    "json.dump(data, open('data.json', 'w'))\n",
    "\n",
    "# Get the top 1000 most controversial posts\n",
    "print(\"CONTROLVERSIAL\")\n",
    "for i, submission in tqdm(enumerate(reddit.subreddit('toastme').controversial(limit=1000)), total=1000):\n",
    "    curr = {}\n",
    "    curr['title'] = submission.title\n",
    "    curr['url'] = submission.url\n",
    "    curr['comments'] = [comment.body for comment in submission.comments if hasattr(comment, 'body')]\n",
    "    data.append(curr)\n",
    "json.dump(data, open('data.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Reddit images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"./images\"\n",
    "if not os.path.exists(image_dir):\n",
    "    os.makedirs(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"data.json\")\n",
    "data[\"ID\"] = [1 + i for i in range(len(data))]\n",
    "count = 1\n",
    "for img, ids in tqdm(zip(data.url, data.ID)):\n",
    "    time.sleep(0.3)\n",
    "    img_name = \"images/ID=\"+ str(ids)+\"_Name=\" +img.split('/', 3)[3]+\".jpg\"\n",
    "    img_name = img_name.replace(\".jpg.jpg\", \".jpg\")\n",
    "    try:\n",
    "        urllib.request.urlretrieve(img, img_name)\n",
    "    except Exception:\n",
    "        pass\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Visual Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from keras.models import Model\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the vgg 19 model is used for getting the image features\n",
    "vgg19_model = VGG19(weights='imagenet')\n",
    "vgg19_model = Model(inputs=vgg19_model.input, outputs=vgg19_model.get_layer('predictions').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get visual features\n",
    "image_files = glob.glob(image_dir+\"/*\")\n",
    "img_features = []\n",
    "\n",
    "for imgf in tqdm(image_files):\n",
    "    img_path = imgf\n",
    "    try:\n",
    "        img = image.load_img(img_path, target_size=(224, 224))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        prediction_features = vgg19_model.predict(x)\n",
    "        img_features.append((imgf, prediction_features.ravel()))\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = pd.read_json(\"data.json\")\n",
    "\n",
    "def extract_id(text):\n",
    "    s = text\n",
    "    start = s.find(image_dir + '/ID=') +13\n",
    "    end = s.find('_Name', start)\n",
    "    return s[start:end]\n",
    "\n",
    "image_features_dataframe = pd.DataFrame(columns = [\"img_name\", \"image_feature\"], data = img_features)\n",
    "image_features_dataframe[\"ID\"] = [int(extract_id(x)) for x in image_features_dataframe.img_name]\n",
    "\n",
    "reddit[\"ID\"] = [1 + i for i in range(len(reddit))]\n",
    "reddit = reddit.merge(image_features_dataframe, on = 'ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the data to be used by ToastBot for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit.to_json(\"data_with_image_features.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
