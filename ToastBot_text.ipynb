{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the ToastBot version that was only trained and evaluated on text data alone. For the complete ToastBot, see `ToastBot.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import os\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from keras.layers.merge import concatenate\n",
    "import IPython\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions\n",
    "Many of these functions were inspired from the following tutorial: \n",
    "https://medium.com/@dev.elect.iitd/neural-machine-translation-using-word-level-seq2seq-model-47538cba8cd7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(filename = \"./data4K_with_image_features.json\", top_comments=10, max_sent=3, max_words=60):\n",
    "    reddit_data = pd.read_json(filename)\n",
    "    \n",
    "    #grab top comments\n",
    "    reddit_data.comments = [x[:top_comments] for x in reddit_data.comments]\n",
    "\n",
    "    # expand the data so that each comment has its own row\n",
    "    lst_col_ga = 'comments'\n",
    "    df = reddit_data\n",
    "    reddit_data = pd.DataFrame({\n",
    "         col:np.repeat(df[col].values, df[lst_col_ga].str.len())\n",
    "         for col in df.columns.drop(lst_col_ga)}\n",
    "       ).assign(**{lst_col_ga:np.concatenate(df[lst_col_ga].values)})[df.columns].dropna().reset_index(drop = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # select relevant columns\n",
    "    reddit_data = reddit_data[[\"title\", \"comments\", \"image_feature\", \"url\"]]\n",
    "    \n",
    "    # clean whitespace\n",
    "    reddit_data.comments = [x.replace(\"\\n\", ' ').replace(\"\\t\", ' ') for x in reddit_data.comments]\n",
    "    reddit_data.title = [x.replace(\"\\n\", ' ').replace(\"\\t\", ' ') for x in reddit_data.title]\n",
    "    \n",
    "    # lower the words\n",
    "    reddit_data.title = reddit_data.title.apply(lambda x: x.lower())\n",
    "    reddit_data.comments = reddit_data.comments.apply(lambda x: x.lower())\n",
    "    \n",
    "    # get only the top sentences\n",
    "    reddit_data.title = [\". \".join(nltk.sent_tokenize(x)[:max_sent]) for x in reddit_data.title]\n",
    "    reddit_data.comments = [\". \".join(nltk.sent_tokenize(x)[:max_sent]) for x in reddit_data.comments]\n",
    "    # if the sentence is too long, cut it off\n",
    "    reddit_data.comments = [\" \".join((nltk.word_tokenize(x))[:max_words]) for x in reddit_data.comments]\n",
    "                 \n",
    "    # remove punctuation\n",
    "    reddit_data.title = reddit_data.title.apply(lambda x: ''.join(ch for ch in x if ch not in set(string.punctuation)))\n",
    "    reddit_data.comments = reddit_data.comments.apply(lambda x: ''.join(ch for ch in \n",
    "                                                                        x if ch not in set(string.punctuation)))\n",
    "    \n",
    "    # remove numbers\n",
    "    remove_numbers = str.maketrans('', '', digits)\n",
    "    reddit_data.title = reddit_data.title.apply(lambda x: x.translate(remove_numbers))\n",
    "    reddit_data.comments = reddit_data.comments.apply(lambda x: x.translate(remove_numbers))\n",
    "                 \n",
    "    # add start and end tokens\n",
    "    reddit_data.comments = reddit_data.comments.apply(lambda x : 'START_ '+ x + ' _END')\n",
    "    \n",
    "    return reddit_data\n",
    "\n",
    "def build_tokenizers(reddit, vocab_size=1000):\n",
    "    # build a vocabulary\n",
    "    title_vocab = [nltk.word_tokenize(x) for x in reddit.title]\n",
    "    title_vocab = [i for k in title_vocab for i in k]\n",
    "    title_vocab = [x[0] for x in Counter(title_vocab).most_common()[:vocab_size]]\n",
    "    comments_vocab = [nltk.word_tokenize(x) for x in reddit.comments]\n",
    "    comments_vocab = [i for k in comments_vocab for i in k]\n",
    "    comments_vocab = [x[0] for x in Counter(comments_vocab).most_common()[:vocab_size]]\n",
    "    \n",
    "    # tokenize\n",
    "    input_words = sorted(list(title_vocab))\n",
    "    target_words = sorted(list(comments_vocab))\n",
    "    input_tokenizer = {word: i for i, word in enumerate(input_words)}\n",
    "    target_tokenizer = {word: i for i, word in enumerate(target_words)}\n",
    "    \n",
    "    return input_tokenizer, target_tokenizer    \n",
    "\n",
    "def get_glove_embeddings(glove_dir):\n",
    "    embeddings_index = {}\n",
    "    with open(os.path.join(glove_dir, 'glove.6B.300d.txt'), encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    print('Found {} word vectors'.format(len(embeddings_index)))\n",
    "    return embeddings_index\n",
    "\n",
    "def build_embedding_matrix(tokenizer, embeddings, embedding_dim = 300):\n",
    "    embedding_matrix = np.zeros((len(tokenizer), embedding_dim))\n",
    "    for word, i in [i for i in tokenizer.items()]:\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "def get_input_target_data(reddit, input_tokenizer, target_tokenizer):\n",
    "    max_comment_length = np.max([len(l.split(' ')) for l in reddit.comments])\n",
    "    max_title_length = np.max([len(l.split(' ')) for l in reddit.title])\n",
    "    \n",
    "    encoder_input_data = np.zeros((len(reddit.title), max_title_length), dtype='float32')\n",
    "    decoder_input_data = np.zeros((len(reddit.comments), max_comment_length), dtype='float32')\n",
    "    decoder_target_data = np.zeros((len(reddit.comments), max_comment_length, \n",
    "                                    len(target_tokenizer)),dtype='float32')\n",
    "    \n",
    "    for i, (input_text, target_text) in enumerate(zip(reddit.title, reddit.comments)):\n",
    "        for t, word in enumerate(input_text.split()):\n",
    "            if word in input_tokenizer:\n",
    "                encoder_input_data[i, t] = input_tokenizer[word]\n",
    "        for t, word in enumerate(target_text.split()):\n",
    "            if word in target_tokenizer: \n",
    "                decoder_input_data[i, t] = target_tokenizer[word]\n",
    "                if t > 0:\n",
    "                    # decoder_target_data will be ahead by one timestep\n",
    "                    decoder_target_data[i, t - 1, target_tokenizer[word]] = 1.\n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data\n",
    "\n",
    "def get_image_features(reddit):\n",
    "    max_comment_length = np.max([len(l.split(' ')) for l in reddit.comments])\n",
    "    image_features = np.array([np.array(i) for i in reddit.image_feature]).reshape(-1, 1, 1000)\n",
    "    image_features = np.broadcast_to(image_features, (image_features.shape[0], \n",
    "                                                      max_comment_length,image_features.shape[2]))\n",
    "    return image_features\n",
    "    \n",
    "def define_model(input_tokenizer, target_tokenizer, embedding_matrix_input, embedding_matrix_target):\n",
    "    # define the encoder\n",
    "    embedding_dim_input = embedding_matrix_input.shape[1] \n",
    "    encoder_inputs = Input(shape=(None,))\n",
    "    en_x =  Embedding(len(input_tokenizer), embedding_dim_input, weights=[embedding_matrix_input], \n",
    "                      trainable = False)(encoder_inputs)\n",
    "    encoder = LSTM(50, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(en_x) #discard output and keep states\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    # define the decoder\n",
    "    embedding_dim_output = embedding_matrix_input.shape[1] \n",
    "    num_decoder_tokens = len(target_tokenizer)\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "    dex =  Embedding(num_decoder_tokens, embedding_dim_output, weights=[embedding_matrix_target], trainable = False)\n",
    "    combined = final_dex\n",
    "\n",
    "    decoder_lstm = LSTM(50, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(combined, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "    # compile the encoder and decoder models for prediction\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    encoder_model.summary()\n",
    "    decoder_state_input_h = Input(shape=(50,))\n",
    "    decoder_state_input_c = Input(shape=(50,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "    final_dex2= dex(decoder_inputs)\n",
    "    combined = final_dex2\n",
    "\n",
    "    decoder_outputs2, state_h2, state_c2 = decoder_lstm(combined, initial_state=decoder_states_inputs)\n",
    "    decoder_states2 = [state_h2, state_c2]\n",
    "    decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "    decoder_model = Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs2] + decoder_states2)\n",
    "    \n",
    "    return model, encoder_model, decoder_model\n",
    "\n",
    "def decode_sequence(input_seq,  encoder_model, decoder_model, input_tokenizer, target_tokenizer):\n",
    "    reverse_input_char_index = dict( (i, char) for char, i in input_tokenizer.items())\n",
    "    reverse_target_char_index = dict((i, char) for char, i in target_tokenizer.items())\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_tokenizer['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence.append(sampled_char)\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_END' or\n",
    "           len(decoded_sentence) > 100):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "        \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, None, 300)         300000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                [(None, 50), (None, 50),  70200     \n",
      "=================================================================\n",
      "Total params: 370,200\n",
      "Trainable params: 70,200\n",
      "Non-trainable params: 300,000\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 300)    300000      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 300)    300000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50), (None,  70200       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 50), ( 70200       embedding_2[0][0]                \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 1000)   51000       lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 791,400\n",
      "Trainable params: 191,400\n",
      "Non-trainable params: 600,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glove_root = \"./glove\"\n",
    "data_path = \"./data_with_image_features.json\"\n",
    "\n",
    "# get the data\n",
    "reddit = process_data(filename = data_path, top_comments=10, max_sent=3, max_words=60)\n",
    "input_tokenizer, target_tokenizer = build_tokenizers(reddit)\n",
    "embeddings = get_glove_embeddings(glove_root)\n",
    "\n",
    "# get embedding matrices\n",
    "input_embedding_matrix = build_embedding_matrix(input_tokenizer, embeddings)\n",
    "output_embedding_matrix = build_embedding_matrix(input_tokenizer, embeddings)\n",
    "\n",
    "#split the data\n",
    "reddit_train, reddit_test = train_test_split(reddit, test_size=0.1, random_state=42)\n",
    "reddit_train = pd.DataFrame(reddit_train, columns = reddit.columns).reset_index(drop = True)\n",
    "reddit_test = pd.DataFrame(reddit_test, columns = reddit.columns).reset_index(drop = True)\n",
    "\n",
    "# get the raw data\n",
    "encoder_input_data_tr, decoder_input_data_tr, decoder_target_data_tr = get_input_target_data(reddit_train,\n",
    "                                                                                             input_tokenizer, \n",
    "                                                                                             target_tokenizer)\n",
    "encoder_input_data_te, decoder_input_data_te, decoder_target_data_te = get_input_target_data(reddit_test, \n",
    "                                                                                             input_tokenizer, \n",
    "                                                                                             target_tokenizer)\n",
    "image_features_tr = get_image_features(reddit_train)\n",
    "image_features_te = get_image_features(reddit_test)\n",
    "\n",
    "\n",
    "# define the model\n",
    "model, encoder_model, decoder_model = define_model(input_tokenizer, target_tokenizer, \n",
    "                                                    input_embedding_matrix, output_embedding_matrix)\n",
    "display(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "model.fit([encoder_input_data_tr, decoder_input_data_tr], decoder_target_data_tr,\n",
    "        validation_data=([encoder_input_data_te, decoder_input_data_te], decoder_target_data_te),\n",
    "          batch_size = 64,\n",
    "          epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"model_text.h5\"\n",
    "model.save_weights(model_name)\n",
    "print(\"Saved model {} to disk\".format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(raw_dataset, decoder_input_data, encoder_input_data, print_examples=False):\n",
    "    inv_target_tokenizer = {v: k for k, v in target_tokenizer.items()}\n",
    "    inv_input_tokenizer = {v: k for k, v in input_tokenizer.items()}\n",
    "    actual, predicted = list(), list()\n",
    "\n",
    "    for seq_index in tqdm(range(len(encoder_input_data))):  \n",
    "        act = [inv_target_tokenizer[i] for i in decoder_input_data_te[seq_index] if inv_target_tokenizer[i] != \"START_\"]\n",
    "        actual.append(act)\n",
    "        \n",
    "        input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "#         input_img = np.array(reddit.image_feature[seq_index]).reshape(1,1,1000)\n",
    "        pred = decode_sequence(input_seq, encoder_model, decoder_model, input_tokenizer, target_tokenizer)\n",
    "        predicted.append(pred)\n",
    "        \n",
    "        if print_examples:\n",
    "            print(\"seq_index: [{}]\".format(seq_index))\n",
    "            title_text = [inv_input_tokenizer[i] for i in encoder_input_data_te[seq_index]]\n",
    "            print(\"input: [{}]\".format(\" \".join(title_text)))\n",
    "            url = raw_dataset.url[seq_index]\n",
    "            display(IPython.display.Image(url, width = 250))\n",
    "            print(\"target: [{}]\".format(\" \".join(act)))\n",
    "            print(\"predicted: [{}] \\n\".format(\" \".join(pred)))\n",
    "            \n",
    "    return corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))\n",
    "\n",
    "def get_one_img(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "\n",
    "    prediction_features = vgg19_model.predict(x)\n",
    "    return prediction_features.ravel()\n",
    "\n",
    "def user_input(input_words, seq_len):\n",
    "    input_seq = np.array([input_tokenizer[w] if w in input_tokenizer else 0 for w in input_words.split()] + \n",
    "                         [0] * (seq_len - len(input_words.split()))).reshape(1, seq_len)\n",
    "#     input_img = np.array(img_feats.reshape(1,1,1000))\n",
    "    inv_input_token_index = {v: k for k, v in input_tokenizer.items()}\n",
    "    #print(\"raw input: [{}] {}\".format(\" \".join([inv_input_token_index[i] for i in input_seq[0]]), input_seq.shape))\n",
    "    decoded_sentence = decode_sequence(input_seq, encoder_model, decoder_model, input_tokenizer, target_tokenizer)\n",
    "    return \" \".join(decoded_sentence).replace(\"START_ \", \"\").replace(\" _END\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"model_text.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(reddit_test, encoder_input_data_te, decoder_input_data_te, print_examples=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
